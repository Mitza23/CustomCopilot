You need to evaluate a RAG system based on an input prompt and the output generated by the system.
You are given a set of 20 prompts, the retrieved chunks and the corresponding output for each input in the input_results.txt file.

Here are details about the evaluation:

Faithfulness
Faithfulness is a RAG metric that evaluates whether the RAG pipeline is generating LLM outputs that factually aligns with
the information presented in the retrieval context. For faithfulness, if you define it as the proportion of truthful claims
made in an LLM output with regards to the retrieval context, we can calculate faithfulness using QAG (Question Answer Generation) Score by following this algorithm:

1. Use LLMs to extract all claims made in the output.
2. For each claim, check whether it agrees or contradicts with each individual node in the retrieval context.
In this case, the close-ended question in QAG will be something like: “Does the given claim agree with the reference text”,
where the “reference text” will be each individual retrieved node. (Note that you need to confine the answer to either a ‘yes’, ‘no’, or ‘idk’.
The ‘idk’ state represents the edge case where the retrieval context does not contain relevant information to give a yes/no answer.)
3. Add up the total number of truthful claims (‘yes’ and ‘idk’), and divide it by the total number of claims made.

Answer Relevancy
Answer relevancy is a RAG metric that assesses whether your RAG generator outputs concise answers, and can be calculated
by determining the proportion of sentences in an LLM output that a relevant to the
input (ie. divide the number relevant sentences by the total number of sentences).

The key to build a robust answer relevancy metric is to take the retrieval context into account, since additional context
may justify a seemingly irrelevant sentence’s relevancy.

Contextual Precision
Contextual Precision is a RAG metric that assesses the quality of your RAG pipeline’s retriever.
When we’re talking about contextual metrics, we’re mainly concerned about the relevancy of the retrieval context.
A high contextual precision score means nodes that are relevant in the retrieval contextual are ranked higher than irrelevant ones.

Contextual Relevancy
Probably the simplest metric to understand, contextual relevancy is simply the proportion of sentences in the retrieval
context that are relevant to a given input.

Compute the faithfulness, answer relevancy, contextual precision, and contextual relevancy scores for the given prompts, their corresponding outputs
and the retrieved context.